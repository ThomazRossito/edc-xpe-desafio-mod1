{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90df9259",
   "metadata": {},
   "source": [
    "# Bootcamp da Xpe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0676d49b",
   "metadata": {},
   "source": [
    "## Engenharia de Dados em Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b99938bb",
   "metadata": {},
   "source": [
    "### Módulo 1: Fundamentos em Arquitetura de Dados e Soluções em Nuvem\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acfda5eb",
   "metadata": {},
   "source": [
    "### Objetivos:\n",
    ">> Implementação de um Data Lake; <br>\n",
    ">> Armazenamento de dados em Storage camada Raw; <br>\n",
    ">> Armazenamento de dados em Storage camada Bronze; <br>\n",
    ">> Armazenamento de dados em Storage camada Silver; <br>\n",
    ">> Implementação de Processamento de Big Data; <br>\n",
    ">> IaC de toda estrutura com Terraform; <br>\n",
    ">> Esteiras de Deploy com Github. <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76ae8fc5",
   "metadata": {},
   "source": [
    "### Esse notebook trata dos itens 2 e 3 do desafio\n",
    "2. Realizar tratamento no dataset da RAIS 2020  <br>\n",
    "    a. Modifique os nomes das colunas, trocando espaços por “_”; <br>\n",
    "    b. Retire acentos e caracter especiais das colunas; <br>\n",
    "    c. Transforme todas as colunas em letras minúsculas; <br>\n",
    "    d. Crie uma coluna “uf” através da coluna \"municipio\"; <br>\n",
    "    e. Realize os ajustes no tipo de dado para as colunas de remuneração.\n",
    "\n",
    "3. Transformar os dados no formato parquet e escrevê-los na zona staging ou zona silver do seu Data Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c0e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75287e6d",
   "metadata": {},
   "source": [
    "#### Inicia uma `Session` do Spark com `Delta Lake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcdf7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create SparkSession\n",
    "spark = (\n",
    "           SparkSession.builder\n",
    "                       .appName(\"DeltaFile\")\n",
    "                       .config(\"spark.sql.shuffle.partitions\", 8)\n",
    "                       .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.0.0\")\n",
    "                       .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "                       .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "                       .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5347dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cloud AWS EMR\n",
    "\n",
    "# %%configure -f \n",
    "\n",
    "# {\n",
    "#   \"conf\": {\n",
    "#     \"spark.jars.packages\": \"io.delta:delta-core_2.12:2.2.0\",\n",
    "#     \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\",\n",
    "#     \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "#     \"spark.streaming.stopGracefullyOnShutdown\": \"true\",\n",
    "#     \"spark.sql.streaming.schemaInference\": \"true\"\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc411f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, input_file_name, regexp_replace\n",
    "from pyspark.sql import functions as spkFn\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Importa o modulo das tabelas delta\n",
    "from delta import *\n",
    "from delta.tables import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e2ef816",
   "metadata": {},
   "source": [
    "#### Path para diretório source e target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c0fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "productRais = \"RAIS2020\"\n",
    "\n",
    "## Path on-Premisses\n",
    "pathRaw     = f\"C:\\\\aws\\\\desafio_mod1\\\\dados\\\\{productRais}\\\\\"\n",
    "pathBronze  = f'C:\\\\aws\\\\desafio_mod1\\\\dados\\\\dados_bronze\\\\delta\\\\{productRais}\\\\'\n",
    "\n",
    "\n",
    "## Ambiente Cloud AWS\n",
    "## Path Cloud AWS\n",
    "# pathRaw    = f\"s3://tarn-datalake-raw-433046906551/RAIS-2020/\"\n",
    "# pathBronze = f\"s3://tarn-datalake-bronze-433046906551/delta/{productRais}/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2f7ff43",
   "metadata": {},
   "source": [
    "#### Schema DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf90e6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/C:/aws/desafio_mod1/dados/RAIS2020/RAIS_VINC_PUB_NORTE.txt.bz2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3412\\2464499260.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfilePathSchema\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{pathRaw}/{fileNameSchehma}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m fileDfSchema    = (spark.read\n\u001b[0m\u001b[0;32m      5\u001b[0m                         \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                         \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thoma\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thoma\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thoma\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/C:/aws/desafio_mod1/dados/RAIS2020/RAIS_VINC_PUB_NORTE.txt.bz2"
     ]
    }
   ],
   "source": [
    "fileNameSchehma = \"RAIS_VINC_PUB_NORTE.txt.bz2\"\n",
    "filePathSchema  = f\"{pathRaw}/{fileNameSchehma}\"\n",
    "\n",
    "fileDfSchema    = (spark.read\n",
    "                        .format(\"csv\")\n",
    "                        .option(\"header\",\"true\")\n",
    "                        .option(\"sep\", \";\")\n",
    "                        .option(\"encoding\", \"latin1\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .load(filePathSchema)\n",
    "                        .schema)\n",
    "\n",
    "schemaJson     = fileDfSchema.json()\n",
    "schemaDDL      = spark.sparkContext._jvm.org.apache.spark.sql.types.DataType.fromJson(schemaJson).toDDL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedaed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Bairros SP` STRING,`Bairros Fortaleza` STRING,`Bairros RJ` STRING,`Causa Afastamento 1` INT,`Causa Afastamento 2` INT,`Causa Afastamento 3` INT,`Motivo Desligamento` INT,`CBO Ocupação 2002` STRING,`CNAE 2.0 Classe` INT,`CNAE 95 Classe` INT,`Distritos SP` STRING,`Vínculo Ativo 31/12` INT,`Faixa Etária` INT,`Faixa Hora Contrat` INT,`Faixa Remun Dezem (SM)` INT,`Faixa Remun Média (SM)` INT,`Faixa Tempo Emprego` INT,`Escolaridade após 2005` DOUBLE,`Qtd Hora Contr` DOUBLE,Idade DOUBLE,`Ind CEI Vinculado` INT,`Ind Simples` INT,`Mês Admissão` INT,`Mês Desligamento` STRING,`Mun Trab` INT,`Município` INT,Nacionalidade INT,`Natureza Jurídica` INT,`Ind Portador Defic` INT,`Qtd Dias Afastamento` DOUBLE,`Raça Cor` INT,`Regiões Adm DF` INT,`Vl Remun Dezembro Nom` STRING,`Vl Remun Dezembro (SM)` STRING,`Vl Remun Média Nom` STRING,`Vl Remun Média (SM)` STRING,`CNAE 2.0 Subclasse` INT,`Sexo Trabalhador` DOUBLE,`Tamanho Estabelecimento` INT,`Tempo Emprego` STRING,`Tipo Admissão` INT,`Tipo Estab41` INT,`Tipo Estab42` STRING,`Tipo Defic` INT,`Tipo Vínculo` INT,`IBGE Subsetor` INT,`Vl Rem Janeiro SC` STRING,`Vl Rem Fevereiro SC` STRING,`Vl Rem Março SC` STRING,`Vl Rem Abril SC` STRING,`Vl Rem Maio SC` STRING,`Vl Rem Junho SC` STRING,`Vl Rem Julho SC` STRING,`Vl Rem Agosto SC` STRING,`Vl Rem Setembro SC` STRING,`Vl Rem Outubro SC` STRING,`Vl Rem Novembro SC` STRING,`Ano Chegada Brasil` INT,`Ind Trab Intermitente` INT,`Ind Trab Parcial` INT\n"
     ]
    }
   ],
   "source": [
    "print(schemaDDL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba09e753",
   "metadata": {},
   "source": [
    "#### Ler arquivos com a API DataFrameRead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read\n",
    "rais2020_csv = (spark.read\n",
    "                     .format(\"csv\")\n",
    "                     .option(\"header\",\"true\")\n",
    "                     .option(\"sep\", \";\")\n",
    "                     .option(\"encoding\", \"latin1\")\n",
    "                     .option(\"inferSchema\", \"true\")\n",
    "                     .schema(schemaDDL)\n",
    "                     .load(pathRaw)\n",
    "                     .withColumn(\"file_name\", lit(input_file_name())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "614643f0",
   "metadata": {},
   "source": [
    "### Funções para normalizar as colunas\n",
    "\n",
    "> ***normalizar_colunas***\n",
    "1. Função para normalizar as colunas de um dataframe\n",
    "    - Retira espaços vazios e incluir um underline (Ex: `Sobre Nome -> Sobre_Nome`)\n",
    "    - Retira ponto e incluir um underline (Ex: `Sobre.Nome -> Sobre_Nome`)\n",
    "    - Formata todas as colunas com letras minúsculas (Ex: `Sobre_Nome -> sobre_nome`)\n",
    "\n",
    "> ***normalizar_acentos***\n",
    "2. Função para retirar os acentos de todas as colunas de um dataframe\n",
    "    - Remover espaços nas extremidades (Ex: `\"  sobre_nome  \" -> \"sobre_nome\"`)\n",
    "    - Replace de carácteres especiais por underline (Ex: `sobre@nome -> sobre_nome`)\n",
    "    - Remover underlines nas extremidades (Ex: `_sobre_nome_ -> sobre_nome`)\n",
    "    - Remover acentuação (Ex: `média_mês -> media_mes`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Função para normalizar as colunas de um dataframe\n",
    "def normalizar_colunas(df):\n",
    "  try:\n",
    "    new_column_spaces_lower = (list(map(lambda x: x.replace(\" \", \"_\")\n",
    "                                                   .replace(\".\", \"_\")\n",
    "                                                   .lower(),\n",
    "                                                 df.columns)))\n",
    "    return df.toDF(*new_column_spaces_lower) \n",
    "  except Exception as err:\n",
    "        error_message = f\"Erro ao normalizar nomes das colunas: {str(err)}\"\n",
    "        print(error_message)\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "## Função para retirar os acentos de todas as colunas de um dataframe\n",
    "def normalizar_acentos(str):\n",
    "  try:\n",
    "    new_str = str\n",
    "    # Remover espaços nas extremidades\n",
    "    new_str = new_str.strip()\n",
    "    # Replace de carácteres especiais por underline\n",
    "    new_str = re.sub(r\"[^\\w]\", \"_\", new_str)\n",
    "    # Remover underlines nas extremidades\n",
    "    new_str = new_str.strip(\"_\")\n",
    "    # Remover acentuação\n",
    "    new_str = unicodedata.normalize('NFKD', new_str)\n",
    "    new_str = u\"\".join([c for c in new_str if not unicodedata.combining(c)])\n",
    "    return new_str\n",
    "  except Exception as err:\n",
    "    error_message = f\"Erro ao normalizar nomes das colunas: {str(err)}\"\n",
    "    print(error_message)\n",
    "    raise ValueError(error_message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a02687b",
   "metadata": {},
   "source": [
    "#### Utiliza as funções no Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ae65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_df       = normalizar_colunas(rais2020_csv)\n",
    "rais2020_renamed = renamed_df.select([spkFn.col(col).alias(normalizar_acentos(col)) for col in renamed_df.columns])\n",
    "\n",
    "print(rais2020_renamed.printSchema())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d892fc35",
   "metadata": {},
   "source": [
    "#### Normalização das colunas de remuneração e outras\n",
    "> - As colunas de `remuneração` <br>\n",
    "    - Utiliza a função `regexp_replace` para fazer um replace de `,` ***vírgula*** para `.` ***ponto*** <br>\n",
    "    - Converte para o tipo de dado `double`\n",
    "    \n",
    "- Cria a coluna `ano` com a informação do ano do dataset\n",
    "- Cria a coluna `uf` com os dois primeiro caracteres da coluna `municipio` e converte para o tipo de dado `inteiro`\n",
    "- Converte a coluna `mes_desligamento` para o tipo de dado `inteiro`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa44194",
   "metadata": {},
   "outputs": [],
   "source": [
    "rais2020_fim = (\n",
    "                 rais2020_renamed\n",
    "                        .withColumn(\"ano\", lit(\"2020\").cast('int'))\n",
    "                        .withColumn(\"uf\", col(\"municipio\").cast('string').substr(1,2).cast('int'))\n",
    "                        .withColumn(\"mes_desligamento\", col('mes_desligamento').cast('int'))\n",
    "                        .withColumn(\"vl_remun_dezembro_nom\", regexp_replace(\"vl_remun_dezembro_nom\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_remun_dezembro_sm\", regexp_replace(\"vl_remun_dezembro__sm\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_remun_media_nom\", regexp_replace(\"vl_remun_media_nom\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_remun_media_sm\", regexp_replace(\"vl_remun_media__sm\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_janeiro_sc\", regexp_replace(\"vl_rem_janeiro_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_fevereiro_sc\", regexp_replace(\"vl_rem_fevereiro_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_marco_sc\", regexp_replace(\"vl_rem_marco_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_abril_sc\", regexp_replace(\"vl_rem_abril_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_maio_sc\", regexp_replace(\"vl_rem_maio_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_junho_sc\", regexp_replace(\"vl_rem_junho_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_julho_sc\", regexp_replace(\"vl_rem_julho_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_agosto_sc\", regexp_replace(\"vl_rem_agosto_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_setembro_sc\", regexp_replace(\"vl_rem_setembro_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_outubro_sc\", regexp_replace(\"vl_rem_outubro_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_novembro_sc\", regexp_replace(\"vl_rem_novembro_sc\", ',', '.').cast('double'))\n",
    "                        .drop(\"vl_remun_dezembro__sm\", \"vl_remun_media__sm\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1148594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    rais2020_fim.write\n",
    "                .format('delta')\n",
    "                .mode('overwrite')\n",
    "                .option('mergeSchema', \"true\")\n",
    "                .partitionBy('ano', 'uf')\n",
    "                .save(pathBronze)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f183be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rais2020_delta = (\n",
    "                   spark.read\n",
    "                        .format('delta')\n",
    "                        .load(pathBronze)\n",
    "                 )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a71d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rais2020_delta.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rais2020_delta.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca9c90c9b299e3c35d28bc96236d8f2c0bd3d51256cb5ad616950692d4a1a879"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
