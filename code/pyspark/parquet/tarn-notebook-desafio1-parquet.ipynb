{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90df9259",
   "metadata": {},
   "source": [
    "# Bootcamp da Xpe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0676d49b",
   "metadata": {},
   "source": [
    "## Engenharia de Dados em Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b99938bb",
   "metadata": {},
   "source": [
    "### Módulo 1: Fundamentos em Arquitetura de Dados e Soluções em Nuvem\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acfda5eb",
   "metadata": {},
   "source": [
    "### Objetivos:\n",
    ">> Implementação de um Data Lake; <br>\n",
    ">> Armazenamento de dados em Storage camada Raw; <br>\n",
    ">> Armazenamento de dados em Storage camada Bronze; <br>\n",
    ">> Armazenamento de dados em Storage camada Silver; <br>\n",
    ">> Implementação de Processamento de Big Data; <br>\n",
    ">> IaC de toda estrutura com Terraform; <br>\n",
    ">> Esteiras de Deploy com Github. <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76ae8fc5",
   "metadata": {},
   "source": [
    "### Esse notebook trata dos itens 2 e 3 do desafio\n",
    "2. Realizar tratamento no dataset da RAIS 2020  <br>\n",
    "    a. Modifique os nomes das colunas, trocando espaços por “_”; <br>\n",
    "    b. Retire acentos e caracter especiais das colunas; <br>\n",
    "    c. Transforme todas as colunas em letras minúsculas; <br>\n",
    "    d. Crie uma coluna “uf” através da coluna \"municipio\"; <br>\n",
    "    e. Realize os ajustes no tipo de dado para as colunas de remuneração.\n",
    "\n",
    "3. Transformar os dados no formato parquet e escrevê-los na zona staging ou zona silver do seu Data Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c0e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, input_file_name, regexp_replace\n",
    "from pyspark.sql import functions as spkFn\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e02b01ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.1\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75287e6d",
   "metadata": {},
   "source": [
    "#### Inicia uma `Session` do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcdf7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .appName(\"readFile\")\n",
    "                     .getOrCreate())  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e2ef816",
   "metadata": {},
   "source": [
    "#### Path para diretório source e target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c0fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "productRais = \"RAIS2020\"\n",
    "pathRaw     = f\"C:/aws/bucket_dow_up/data_up/{productRais}/\"\n",
    "pathBronze  = f'C:/aws/dados/dados_bronze/{productRais}/parquet/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2f7ff43",
   "metadata": {},
   "source": [
    "#### Schema DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cf90e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNameSchema = \"RAIS_VINC_PUB_NORTE.txt.bz2\"\n",
    "filePathSchema = f\"{pathRaw}/{fileNameSchema}\"\n",
    "\n",
    "fileDfSchema   = (spark.read\n",
    "                       .format(\"csv\")\n",
    "                       .option(\"header\",\"true\")\n",
    "                       .option(\"sep\", \";\")\n",
    "                       .option(\"encoding\", \"latin1\")\n",
    "                       .option(\"inferSchema\", \"true\")\n",
    "                       .load(filePathSchema)\n",
    "                       .schema)\n",
    "\n",
    "schemaJson     = fileDfSchema.json()\n",
    "schemaDDL      = spark.sparkContext._jvm.org.apache.spark.sql.types.DataType.fromJson(schemaJson).toDDL()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba09e753",
   "metadata": {},
   "source": [
    "#### Ler arquivos com a API DataFrameRead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c496cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read\n",
    "rais2020_csv = (spark.read\n",
    "                     .format(\"csv\")\n",
    "                     .option(\"header\",\"true\")\n",
    "                     .option(\"sep\", \";\")\n",
    "                     .option(\"encoding\", \"latin1\")\n",
    "                     .option(\"inferSchema\", \"true\")\n",
    "                     .schema(schemaDDL)\n",
    "                     .load(pathRaw)\n",
    "                     .withColumn(\"file_name\", lit(input_file_name())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b721058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Bairros SP: string (nullable = true)\n",
      " |-- Bairros Fortaleza: string (nullable = true)\n",
      " |-- Bairros RJ: string (nullable = true)\n",
      " |-- Causa Afastamento 1: integer (nullable = true)\n",
      " |-- Causa Afastamento 2: integer (nullable = true)\n",
      " |-- Causa Afastamento 3: integer (nullable = true)\n",
      " |-- Motivo Desligamento: integer (nullable = true)\n",
      " |-- CBO Ocupação 2002: string (nullable = true)\n",
      " |-- CNAE 2.0 Classe: integer (nullable = true)\n",
      " |-- CNAE 95 Classe: integer (nullable = true)\n",
      " |-- Distritos SP: string (nullable = true)\n",
      " |-- Vínculo Ativo 31/12: integer (nullable = true)\n",
      " |-- Faixa Etária: integer (nullable = true)\n",
      " |-- Faixa Hora Contrat: integer (nullable = true)\n",
      " |-- Faixa Remun Dezem (SM): integer (nullable = true)\n",
      " |-- Faixa Remun Média (SM): integer (nullable = true)\n",
      " |-- Faixa Tempo Emprego: integer (nullable = true)\n",
      " |-- Escolaridade após 2005: double (nullable = true)\n",
      " |-- Qtd Hora Contr: double (nullable = true)\n",
      " |-- Idade: double (nullable = true)\n",
      " |-- Ind CEI Vinculado: integer (nullable = true)\n",
      " |-- Ind Simples: integer (nullable = true)\n",
      " |-- Mês Admissão: integer (nullable = true)\n",
      " |-- Mês Desligamento: string (nullable = true)\n",
      " |-- Mun Trab: integer (nullable = true)\n",
      " |-- Município: integer (nullable = true)\n",
      " |-- Nacionalidade: integer (nullable = true)\n",
      " |-- Natureza Jurídica: integer (nullable = true)\n",
      " |-- Ind Portador Defic: integer (nullable = true)\n",
      " |-- Qtd Dias Afastamento: double (nullable = true)\n",
      " |-- Raça Cor: integer (nullable = true)\n",
      " |-- Regiões Adm DF: integer (nullable = true)\n",
      " |-- Vl Remun Dezembro Nom: string (nullable = true)\n",
      " |-- Vl Remun Dezembro (SM): string (nullable = true)\n",
      " |-- Vl Remun Média Nom: string (nullable = true)\n",
      " |-- Vl Remun Média (SM): string (nullable = true)\n",
      " |-- CNAE 2.0 Subclasse: integer (nullable = true)\n",
      " |-- Sexo Trabalhador: double (nullable = true)\n",
      " |-- Tamanho Estabelecimento: integer (nullable = true)\n",
      " |-- Tempo Emprego: string (nullable = true)\n",
      " |-- Tipo Admissão: integer (nullable = true)\n",
      " |-- Tipo Estab41: integer (nullable = true)\n",
      " |-- Tipo Estab42: string (nullable = true)\n",
      " |-- Tipo Defic: integer (nullable = true)\n",
      " |-- Tipo Vínculo: integer (nullable = true)\n",
      " |-- IBGE Subsetor: integer (nullable = true)\n",
      " |-- Vl Rem Janeiro SC: string (nullable = true)\n",
      " |-- Vl Rem Fevereiro SC: string (nullable = true)\n",
      " |-- Vl Rem Março SC: string (nullable = true)\n",
      " |-- Vl Rem Abril SC: string (nullable = true)\n",
      " |-- Vl Rem Maio SC: string (nullable = true)\n",
      " |-- Vl Rem Junho SC: string (nullable = true)\n",
      " |-- Vl Rem Julho SC: string (nullable = true)\n",
      " |-- Vl Rem Agosto SC: string (nullable = true)\n",
      " |-- Vl Rem Setembro SC: string (nullable = true)\n",
      " |-- Vl Rem Outubro SC: string (nullable = true)\n",
      " |-- Vl Rem Novembro SC: string (nullable = true)\n",
      " |-- Ano Chegada Brasil: integer (nullable = true)\n",
      " |-- Ind Trab Intermitente: integer (nullable = true)\n",
      " |-- Ind Trab Parcial: integer (nullable = true)\n",
      " |-- file_name: string (nullable = false)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(rais2020_csv.printSchema())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "614643f0",
   "metadata": {},
   "source": [
    "### Funções para normalizar as colunas\n",
    "\n",
    "> ***normalizar_colunas***\n",
    "1. Função para normalizar as colunas de um dataframe\n",
    "    - Retira espaços vazios e incluir um underline (Ex: `Sobre Nome -> Sobre_Nome`)\n",
    "    - Retira ponto e incluir um underline (Ex: `Sobre.Nome -> Sobre_Nome`)\n",
    "    - Formata todas as colunas com letras minúsculas (Ex: `Sobre_Nome -> sobre_nome`)\n",
    "\n",
    "> ***normalizar_acentos***\n",
    "2. Função para retirar os acentos de todas as colunas de um dataframe\n",
    "    - Remover espaços nas extremidades (Ex: `\"  sobre_nome  \" -> \"sobre_nome\"`)\n",
    "    - Replace de carácteres especiais por underline (Ex: `sobre@nome -> sobre_nome`)\n",
    "    - Remover underlines nas extremidades (Ex: `_sobre_nome_ -> sobre_nome`)\n",
    "    - Remover acentuação (Ex: `média_mês -> media_mes`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "708a3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Função para normalizar as colunas de um dataframe\n",
    "def normalizar_colunas(df):\n",
    "  try:\n",
    "    new_column_spaces_lower = (list(map(lambda x: x.replace(\" \", \"_\")\n",
    "                                                   .replace(\".\", \"_\")\n",
    "                                                   .lower(),\n",
    "                                                 df.columns)))\n",
    "    return df.toDF(*new_column_spaces_lower) \n",
    "  except Exception as err:\n",
    "        error_message = f\"Erro ao normalizar nomes das colunas: {str(err)}\"\n",
    "        print(error_message)\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "## Função para retirar os acentos de todas as colunas de um dataframe\n",
    "def normalizar_acentos(str):\n",
    "  try:\n",
    "    new_str = str\n",
    "    # Remover espaços nas extremidades\n",
    "    new_str = new_str.strip()\n",
    "    # Replace de carácteres especiais por underline\n",
    "    new_str = re.sub(r\"[^\\w]\", \"_\", new_str)\n",
    "    # Remover underlines nas extremidades\n",
    "    new_str = new_str.strip(\"_\")\n",
    "    # Remover 2 underlines juntos e deixar apenas 1\n",
    "    new_str = new_str.replace(\"__\", \"_\")\n",
    "    # Remover acentuação\n",
    "    new_str = unicodedata.normalize('NFKD', new_str)\n",
    "    new_str = u\"\".join([c for c in new_str if not unicodedata.combining(c)])\n",
    "    return new_str\n",
    "  except Exception as err:\n",
    "    error_message = f\"Erro ao normalizar nomes das colunas: {str(err)}\"\n",
    "    print(error_message)\n",
    "    raise ValueError(error_message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a02687b",
   "metadata": {},
   "source": [
    "#### Utiliza as funções no Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "767ae65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_df = normalizar_colunas(rais2020_csv)\n",
    "\n",
    "rais2020_renamed = renamed_df.select([spkFn.col(col).alias(normalizar_acentos(col)) for col in renamed_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f8fcfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bairros_sp: string (nullable = true)\n",
      " |-- bairros_fortaleza: string (nullable = true)\n",
      " |-- bairros_rj: string (nullable = true)\n",
      " |-- causa_afastamento_1: integer (nullable = true)\n",
      " |-- causa_afastamento_2: integer (nullable = true)\n",
      " |-- causa_afastamento_3: integer (nullable = true)\n",
      " |-- motivo_desligamento: integer (nullable = true)\n",
      " |-- cbo_ocupacao_2002: string (nullable = true)\n",
      " |-- cnae_2_0_classe: integer (nullable = true)\n",
      " |-- cnae_95_classe: integer (nullable = true)\n",
      " |-- distritos_sp: string (nullable = true)\n",
      " |-- vinculo_ativo_31_12: integer (nullable = true)\n",
      " |-- faixa_etaria: integer (nullable = true)\n",
      " |-- faixa_hora_contrat: integer (nullable = true)\n",
      " |-- faixa_remun_dezem_sm: integer (nullable = true)\n",
      " |-- faixa_remun_media_sm: integer (nullable = true)\n",
      " |-- faixa_tempo_emprego: integer (nullable = true)\n",
      " |-- escolaridade_apos_2005: double (nullable = true)\n",
      " |-- qtd_hora_contr: double (nullable = true)\n",
      " |-- idade: double (nullable = true)\n",
      " |-- ind_cei_vinculado: integer (nullable = true)\n",
      " |-- ind_simples: integer (nullable = true)\n",
      " |-- mes_admissao: integer (nullable = true)\n",
      " |-- mes_desligamento: string (nullable = true)\n",
      " |-- mun_trab: integer (nullable = true)\n",
      " |-- municipio: integer (nullable = true)\n",
      " |-- nacionalidade: integer (nullable = true)\n",
      " |-- natureza_juridica: integer (nullable = true)\n",
      " |-- ind_portador_defic: integer (nullable = true)\n",
      " |-- qtd_dias_afastamento: double (nullable = true)\n",
      " |-- raca_cor: integer (nullable = true)\n",
      " |-- regioes_adm_df: integer (nullable = true)\n",
      " |-- vl_remun_dezembro_nom: string (nullable = true)\n",
      " |-- vl_remun_dezembro_sm: string (nullable = true)\n",
      " |-- vl_remun_media_nom: string (nullable = true)\n",
      " |-- vl_remun_media_sm: string (nullable = true)\n",
      " |-- cnae_2_0_subclasse: integer (nullable = true)\n",
      " |-- sexo_trabalhador: double (nullable = true)\n",
      " |-- tamanho_estabelecimento: integer (nullable = true)\n",
      " |-- tempo_emprego: string (nullable = true)\n",
      " |-- tipo_admissao: integer (nullable = true)\n",
      " |-- tipo_estab41: integer (nullable = true)\n",
      " |-- tipo_estab42: string (nullable = true)\n",
      " |-- tipo_defic: integer (nullable = true)\n",
      " |-- tipo_vinculo: integer (nullable = true)\n",
      " |-- ibge_subsetor: integer (nullable = true)\n",
      " |-- vl_rem_janeiro_sc: string (nullable = true)\n",
      " |-- vl_rem_fevereiro_sc: string (nullable = true)\n",
      " |-- vl_rem_marco_sc: string (nullable = true)\n",
      " |-- vl_rem_abril_sc: string (nullable = true)\n",
      " |-- vl_rem_maio_sc: string (nullable = true)\n",
      " |-- vl_rem_junho_sc: string (nullable = true)\n",
      " |-- vl_rem_julho_sc: string (nullable = true)\n",
      " |-- vl_rem_agosto_sc: string (nullable = true)\n",
      " |-- vl_rem_setembro_sc: string (nullable = true)\n",
      " |-- vl_rem_outubro_sc: string (nullable = true)\n",
      " |-- vl_rem_novembro_sc: string (nullable = true)\n",
      " |-- ano_chegada_brasil: integer (nullable = true)\n",
      " |-- ind_trab_intermitente: integer (nullable = true)\n",
      " |-- ind_trab_parcial: integer (nullable = true)\n",
      " |-- file_name: string (nullable = false)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(rais2020_renamed.printSchema())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d892fc35",
   "metadata": {},
   "source": [
    "#### Normalização das colunas de remuneração e outras\n",
    "> - As colunas de `remuneração` <br>\n",
    "    - Utiliza a função `regexp_replace` para fazer um replace de `,` ***vírgula*** para `.` ***ponto*** <br>\n",
    "    - Converte para o tipo de dado `double`\n",
    "    \n",
    "- Cria a coluna `ano` com a informação do ano do dataset\n",
    "- Cria a coluna `uf` com os dois primeiro caracteres da coluna `municipio` e converte para o tipo de dado `inteiro`\n",
    "- Converte a coluna `mes_desligamento` para o tipo de dado `inteiro`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffa44194",
   "metadata": {},
   "outputs": [],
   "source": [
    "rais2020_fim = (\n",
    "                 rais2020_renamed\n",
    "                        .withColumn(\"ano\", lit(\"2020\").cast('int'))\n",
    "                        .withColumn(\"uf\", col(\"municipio\").cast('string').substr(1,2).cast('int'))\n",
    "                        .withColumn(\"mes_desligamento\", col('mes_desligamento').cast('int'))\n",
    "                        .withColumn(\"vl_remun_dezembro_nom\", regexp_replace(\"vl_remun_dezembro_nom\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_remun_dezembro_sm\", regexp_replace(\"vl_remun_dezembro_sm\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_remun_media_nom\", regexp_replace(\"vl_remun_media_nom\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_remun_media_sm\", regexp_replace(\"vl_remun_media_sm\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_janeiro_sc\", regexp_replace(\"vl_rem_janeiro_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_fevereiro_sc\", regexp_replace(\"vl_rem_fevereiro_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_marco_sc\", regexp_replace(\"vl_rem_marco_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_abril_sc\", regexp_replace(\"vl_rem_abril_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_maio_sc\", regexp_replace(\"vl_rem_maio_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_junho_sc\", regexp_replace(\"vl_rem_junho_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_julho_sc\", regexp_replace(\"vl_rem_julho_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_agosto_sc\", regexp_replace(\"vl_rem_agosto_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_setembro_sc\", regexp_replace(\"vl_rem_setembro_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_outubro_sc\", regexp_replace(\"vl_rem_outubro_sc\", ',', '.').cast('double'))\n",
    "                        .withColumn(\"vl_rem_novembro_sc\", regexp_replace(\"vl_rem_novembro_sc\", ',', '.').cast('double'))\n",
    "                        .drop(\"vl_remun_dezembro__sm\", \"vl_remun_media__sm\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1148594c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1066.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 10) (DELL-GAMER executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\r\n\tat java.lang.StringCoding.safeTrim(StringCoding.java:79)\r\n\tat java.lang.StringCoding.encode(StringCoding.java:365)\r\n\tat java.lang.String.getBytes(String.java:941)\r\n\tat org.apache.spark.unsafe.types.UTF8String.fromString(UTF8String.java:139)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$26(UnivocityParser.scala:224)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$Lambda$3247/807350816.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:259)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$25(UnivocityParser.scala:224)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$Lambda$3203/810818832.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:312)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$Lambda$3227/2002008724.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$$Lambda$3241/883053958.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$$Lambda$3244/1160499787.apply(Unknown Source)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\r\n\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\r\n\tat org.apache.spark.sql.execution.SortExec$$Lambda$3071/2077647346.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2553/1302281177.apply(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\r\n\t... 41 more\r\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\r\n\tat java.lang.StringCoding.safeTrim(StringCoding.java:79)\r\n\tat java.lang.StringCoding.encode(StringCoding.java:365)\r\n\tat java.lang.String.getBytes(String.java:941)\r\n\tat org.apache.spark.unsafe.types.UTF8String.fromString(UTF8String.java:139)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$26(UnivocityParser.scala:224)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$Lambda$3247/807350816.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:259)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$25(UnivocityParser.scala:224)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$Lambda$3203/810818832.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:312)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$Lambda$3227/2002008724.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$$Lambda$3241/883053958.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$$Lambda$3244/1160499787.apply(Unknown Source)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\r\n\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\r\n\tat org.apache.spark.sql.execution.SortExec$$Lambda$3071/2077647346.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2553/1302281177.apply(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7124\\8498575.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m (\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mrais2020_fim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'overwrite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ano'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'uf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thoma\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    966\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thoma\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thoma\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\thoma\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1066.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 10) (DELL-GAMER executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\r\n\tat java.lang.StringCoding.safeTrim(StringCoding.java:79)\r\n\tat java.lang.StringCoding.encode(StringCoding.java:365)\r\n\tat java.lang.String.getBytes(String.java:941)\r\n\tat org.apache.spark.unsafe.types.UTF8String.fromString(UTF8String.java:139)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$26(UnivocityParser.scala:224)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$Lambda$3247/807350816.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:259)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$25(UnivocityParser.scala:224)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$Lambda$3203/810818832.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:312)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$Lambda$3227/2002008724.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$$Lambda$3241/883053958.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$$Lambda$3244/1160499787.apply(Unknown Source)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\r\n\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\r\n\tat org.apache.spark.sql.execution.SortExec$$Lambda$3071/2077647346.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2553/1302281177.apply(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\r\n\t... 41 more\r\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\r\n\tat java.lang.StringCoding.safeTrim(StringCoding.java:79)\r\n\tat java.lang.StringCoding.encode(StringCoding.java:365)\r\n\tat java.lang.String.getBytes(String.java:941)\r\n\tat org.apache.spark.unsafe.types.UTF8String.fromString(UTF8String.java:139)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$26(UnivocityParser.scala:224)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$Lambda$3247/807350816.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:259)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$25(UnivocityParser.scala:224)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$Lambda$3203/810818832.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:312)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$Lambda$3227/2002008724.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$$Lambda$3241/883053958.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$$$Lambda$3244/1160499787.apply(Unknown Source)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\r\n\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\r\n\tat org.apache.spark.sql.execution.SortExec$$Lambda$3071/2077647346.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2553/1302281177.apply(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    rais2020_fim.write\n",
    "                .format('parquet')\n",
    "                .mode('overwrite')\n",
    "                .partitionBy('ano', 'uf')\n",
    "                .save(pathBronze)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f183be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rais2020_parquet = (\n",
    "                      spark.read\n",
    "                           .format('parquet')\n",
    "                           .load(pathBronze)\n",
    "                   )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a71d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74117924"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rais2020_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278c29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bairros_sp: string (nullable = true)\n",
      " |-- bairros_fortaleza: string (nullable = true)\n",
      " |-- bairros_rj: string (nullable = true)\n",
      " |-- causa_afastamento_1: integer (nullable = true)\n",
      " |-- causa_afastamento_2: integer (nullable = true)\n",
      " |-- causa_afastamento_3: integer (nullable = true)\n",
      " |-- motivo_desligamento: integer (nullable = true)\n",
      " |-- cbo_ocupacao_2002: string (nullable = true)\n",
      " |-- cnae_2_0_classe: integer (nullable = true)\n",
      " |-- cnae_95_classe: integer (nullable = true)\n",
      " |-- distritos_sp: string (nullable = true)\n",
      " |-- vinculo_ativo_31_12: integer (nullable = true)\n",
      " |-- faixa_etaria: integer (nullable = true)\n",
      " |-- faixa_hora_contrat: integer (nullable = true)\n",
      " |-- faixa_remun_dezem__sm: integer (nullable = true)\n",
      " |-- faixa_remun_media__sm: integer (nullable = true)\n",
      " |-- faixa_tempo_emprego: integer (nullable = true)\n",
      " |-- escolaridade_apos_2005: double (nullable = true)\n",
      " |-- qtd_hora_contr: double (nullable = true)\n",
      " |-- idade: double (nullable = true)\n",
      " |-- ind_cei_vinculado: integer (nullable = true)\n",
      " |-- ind_simples: integer (nullable = true)\n",
      " |-- mes_admissao: integer (nullable = true)\n",
      " |-- mes_desligamento: integer (nullable = true)\n",
      " |-- mun_trab: integer (nullable = true)\n",
      " |-- municipio: integer (nullable = true)\n",
      " |-- nacionalidade: integer (nullable = true)\n",
      " |-- natureza_juridica: integer (nullable = true)\n",
      " |-- ind_portador_defic: integer (nullable = true)\n",
      " |-- qtd_dias_afastamento: double (nullable = true)\n",
      " |-- raca_cor: integer (nullable = true)\n",
      " |-- regioes_adm_df: integer (nullable = true)\n",
      " |-- vl_remun_dezembro_nom: double (nullable = true)\n",
      " |-- vl_remun_media_nom: double (nullable = true)\n",
      " |-- cnae_2_0_subclasse: integer (nullable = true)\n",
      " |-- sexo_trabalhador: double (nullable = true)\n",
      " |-- tamanho_estabelecimento: integer (nullable = true)\n",
      " |-- tempo_emprego: string (nullable = true)\n",
      " |-- tipo_admissao: integer (nullable = true)\n",
      " |-- tipo_estab41: integer (nullable = true)\n",
      " |-- tipo_estab42: string (nullable = true)\n",
      " |-- tipo_defic: integer (nullable = true)\n",
      " |-- tipo_vinculo: integer (nullable = true)\n",
      " |-- ibge_subsetor: integer (nullable = true)\n",
      " |-- vl_rem_janeiro_sc: double (nullable = true)\n",
      " |-- vl_rem_fevereiro_sc: double (nullable = true)\n",
      " |-- vl_rem_marco_sc: double (nullable = true)\n",
      " |-- vl_rem_abril_sc: double (nullable = true)\n",
      " |-- vl_rem_maio_sc: double (nullable = true)\n",
      " |-- vl_rem_junho_sc: double (nullable = true)\n",
      " |-- vl_rem_julho_sc: double (nullable = true)\n",
      " |-- vl_rem_agosto_sc: double (nullable = true)\n",
      " |-- vl_rem_setembro_sc: double (nullable = true)\n",
      " |-- vl_rem_outubro_sc: double (nullable = true)\n",
      " |-- vl_rem_novembro_sc: double (nullable = true)\n",
      " |-- ano_chegada_brasil: integer (nullable = true)\n",
      " |-- ind_trab_intermitente: integer (nullable = true)\n",
      " |-- ind_trab_parcial: integer (nullable = true)\n",
      " |-- file_name: string (nullable = true)\n",
      " |-- vl_remun_dezembro_sm: double (nullable = true)\n",
      " |-- vl_remun_media_sm: double (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- uf: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rais2020_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7ee74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca9c90c9b299e3c35d28bc96236d8f2c0bd3d51256cb5ad616950692d4a1a879"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
